{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98461fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import gensim.parsing.preprocessing as gen_preproc\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f65e11",
   "metadata": {},
   "source": [
    "# Data Import \n",
    "The same stuff I've done in all the file / models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "656793e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>basicProc</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>the rock is destined to be the 21st century s ...</td>\n",
       "      <td>the rock is destin to be the 21st centuri s ne...</td>\n",
       "      <td>the rock be destine to be the 21st century s n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "      <td>the gorgeous elabor continu of the lord of the...</td>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "      <td>singer composer bryan adams contributes a slew...</td>\n",
       "      <td>singer compos bryan adam contribut a slew of s...</td>\n",
       "      <td>singer composer bryan adams contribute a slew ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>you d think by now america would have had enou...</td>\n",
       "      <td>you d think by now america would have had enou...</td>\n",
       "      <td>-PRON- d think by now america would have have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>yet the act is still charming here</td>\n",
       "      <td>yet the act is still charm here</td>\n",
       "      <td>yet the act be still charm here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      4  The Rock is destined to be the 21st Century 's...   \n",
       "1      5  The gorgeously elaborate continuation of `` Th...   \n",
       "2      4  Singer/composer Bryan Adams contributes a slew...   \n",
       "3      3  You 'd think by now America would have had eno...   \n",
       "4      4               Yet the act is still charming here .   \n",
       "\n",
       "                                           basicProc  \\\n",
       "0  the rock is destined to be the 21st century s ...   \n",
       "1  the gorgeously elaborate continuation of the l...   \n",
       "2  singer composer bryan adams contributes a slew...   \n",
       "3  you d think by now america would have had enou...   \n",
       "4                 yet the act is still charming here   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  the rock is destin to be the 21st centuri s ne...   \n",
       "1  the gorgeous elabor continu of the lord of the...   \n",
       "2  singer compos bryan adam contribut a slew of s...   \n",
       "3  you d think by now america would have had enou...   \n",
       "4                    yet the act is still charm here   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  the rock be destine to be the 21st century s n...  \n",
       "1  the gorgeously elaborate continuation of the l...  \n",
       "2  singer composer bryan adams contribute a slew ...  \n",
       "3  -PRON- d think by now america would have have ...  \n",
       "4                    yet the act be still charm here  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv( 'parsed_train.txt', sep='\\t', header=None,\n",
    "                   names=['label', 'text'] )\n",
    "df['label'] = df['label'].apply(lambda x: int( x.replace(\"__label__\", \"\") ) )\n",
    "\n",
    "basicPreproc = [lambda x: x.lower(), gen_preproc.strip_tags, \\\n",
    "    gen_preproc.strip_punctuation, gen_preproc.strip_non_alphanum, \\\n",
    "    gen_preproc.strip_multiple_whitespaces] #, gen_preproc.strip_short, gen_preproc.remove_stopwords\n",
    "\n",
    "df['basicProc'] = df['text'].apply(lambda x: ' '.join( gen_preproc.preprocess_string(x, basicPreproc)))\n",
    "df['stemmed'] = df['basicProc'].apply(lambda x: gen_preproc.stem_text(x))\n",
    "df['lemmatized'] = df['basicProc'].apply(lambda x: ' '.join( [token.lemma_ for token in nlp(x)] ) )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07b9c5",
   "metadata": {},
   "source": [
    "### Test Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068d316b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>basicProc</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>effective but too tepid biopic</td>\n",
       "      <td>effect but too tepid biopic</td>\n",
       "      <td>effective but too tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>if you sometim like to go to the movi to have ...</td>\n",
       "      <td>if -PRON- sometimes like to go to the movie to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>emerges as something rare an issue movie that ...</td>\n",
       "      <td>emerg as someth rare an issu movi that s so ho...</td>\n",
       "      <td>emerge as something rare an issue movie that s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The film provides some great insight into the ...</td>\n",
       "      <td>the film provides some great insight into the ...</td>\n",
       "      <td>the film provid some great insight into the ne...</td>\n",
       "      <td>the film provide some great insight into the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>offers that rare combination of entertainment ...</td>\n",
       "      <td>offer that rare combin of entertain and educ</td>\n",
       "      <td>offer that rare combination of entertainment a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      3                     Effective but too-tepid biopic   \n",
       "1      4  If you sometimes like to go to the movies to h...   \n",
       "2      5  Emerges as something rare , an issue movie tha...   \n",
       "3      3  The film provides some great insight into the ...   \n",
       "4      5  Offers that rare combination of entertainment ...   \n",
       "\n",
       "                                           basicProc  \\\n",
       "0                     effective but too tepid biopic   \n",
       "1  if you sometimes like to go to the movies to h...   \n",
       "2  emerges as something rare an issue movie that ...   \n",
       "3  the film provides some great insight into the ...   \n",
       "4  offers that rare combination of entertainment ...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0                        effect but too tepid biopic   \n",
       "1  if you sometim like to go to the movi to have ...   \n",
       "2  emerg as someth rare an issu movi that s so ho...   \n",
       "3  the film provid some great insight into the ne...   \n",
       "4       offer that rare combin of entertain and educ   \n",
       "\n",
       "                                          lemmatized  \n",
       "0                     effective but too tepid biopic  \n",
       "1  if -PRON- sometimes like to go to the movie to...  \n",
       "2  emerge as something rare an issue movie that s...  \n",
       "3  the film provide some great insight into the n...  \n",
       "4  offer that rare combination of entertainment a...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv( 'parsed_test.txt', sep='\\t', header=None,\n",
    "                   names=['label', 'text'] )\n",
    "test_df['label'] = test_df['label'].apply(lambda x: int( x.replace(\"__label__\", \"\") ) )\n",
    "test_df['basicProc'] = test_df['text'].apply(lambda x: ' '.join( gen_preproc.preprocess_string(x, basicPreproc)))\n",
    "test_df['stemmed'] = test_df['basicProc'].apply(lambda x: gen_preproc.stem_text(x))\n",
    "test_df['lemmatized'] = test_df['basicProc'].apply(lambda x: ' '.join( [token.lemma_ for token in nlp(x)] ) )\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42f80c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bert\n",
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = \\\n",
    "    ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b76255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 4132928.12B/s]\n",
      "100%|██████████| 442/442 [00:00<00:00, 220542.75B/s]\n",
      "100%|██████████| 267967963/267967963 [00:17<00:00, 15583598.05B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9202358",
   "metadata": {},
   "source": [
    "### Prep BERT Input Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bc6cfe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1996,\n",
       " 2600,\n",
       " 2022,\n",
       " 4078,\n",
       " 10196,\n",
       " 2000,\n",
       " 2022,\n",
       " 1996,\n",
       " 7398,\n",
       " 2301,\n",
       " 1055,\n",
       " 2047,\n",
       " 16608,\n",
       " 1998,\n",
       " 2008,\n",
       " 1011,\n",
       " 4013,\n",
       " 2078,\n",
       " 1011,\n",
       " 1055,\n",
       " 2175,\n",
       " 2000,\n",
       " 2191,\n",
       " 1037,\n",
       " 17624,\n",
       " 2130,\n",
       " 2307,\n",
       " 2084,\n",
       " 7779,\n",
       " 29058,\n",
       " 8625,\n",
       " 13327,\n",
       " 3744,\n",
       " 18856,\n",
       " 19513,\n",
       " 3158,\n",
       " 5477,\n",
       " 4168,\n",
       " 2030,\n",
       " 7112,\n",
       " 16562,\n",
       " 2140,\n",
       " 102]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = df['lemmatized'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b23e754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544, 71)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Padding where everything is padded to the max length. Could use other functions for padding as well.\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bce8767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  1996,  2600,  2022,  4078, 10196,  2000,  2022,  1996,\n",
       "        7398,  2301,  1055,  2047, 16608,  1998,  2008,  1011,  4013,\n",
       "        2078,  1011,  1055,  2175,  2000,  2191,  1037, 17624,  2130,\n",
       "        2307,  2084,  7779, 29058,  8625, 13327,  3744, 18856, 19513,\n",
       "        3158,  5477,  4168,  2030,  7112, 16562,  2140,   102,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac1826",
   "metadata": {},
   "source": [
    "Masked features tell BERT to Ignore the padding bits. This is important b.c. it means we're not diluting our information when padding, and that having long padding doesn't matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a569f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544, 71)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75d3e2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c2a3a1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7454195712 bytes. Buy new RAM!\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1/ipykernel_17036/675023364.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mlast_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\LSTMsPytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\LSTMsPytorch\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# (bs, seq_length, dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m         tfmr_output = self.transformer(x=embedding_output,\n\u001b[0m\u001b[0;32m    463\u001b[0m                                        \u001b[0mattn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m                                        head_mask=head_mask)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\LSTMsPytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\LSTMsPytorch\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, attn_mask, head_mask)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             layer_outputs = layer_module(x=hidden_state,\n\u001b[0m\u001b[0;32m    304\u001b[0m                                          \u001b[0mattn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                                          head_mask=head_mask[i])\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\LSTMsPytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\LSTMsPytorch\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, attn_mask, head_mask)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;31m# Feed Forward Network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mffn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[1;33m)\u001b[0m                             \u001b[1;31m# (bs, seq_length, dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_layer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msa_output\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bs, seq_length, dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\LSTMsPytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\LSTMsPytorch\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\LSTMsPytorch\\lib\\site-packages\\transformers\\modeling_distilbert.py\u001b[0m in \u001b[0;36mgelu\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m### UTILS AND BUILDING BLOCKS OF THE ARCHITECTURE ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_sinusoidal_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7454195712 bytes. Buy new RAM!\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(padded)\n",
    "input_ids = input_ids.to(input_ids.device).long() \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "attention_mask = attention_mask.to(attention_mask.device).long()\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ffa766",
   "metadata": {},
   "source": [
    "# Not Enough RAM hahaha\n",
    "Ok... lets cut it down into chunks. I have ~32 GB ram. That used only 7.5 gb if my math is correct? Maybe Jupyter has a max allocated to it? Note that numpy array split will keep the indexing the same, so don't needa zip & unzip stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3304a830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.454195712"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7454195712 / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b1d58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedChunks = np.array_split( padded, 5 )\n",
    "attentionChunks = np.array_split( attention_mask, 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cede660f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1709, 71)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddedChunks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e3f00b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(paddedChunk, attentionChunk):\n",
    "    input_ids = torch.tensor(paddedChunk)\n",
    "    input_ids = input_ids.to(input_ids.device).long() \n",
    "    attention_mask = torch.tensor(attentionChunk)\n",
    "    attention_mask = attention_mask.to(attention_mask.device).long()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "    return last_hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ea69689",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states_0 = get_hidden_states(paddedChunks[0], attentionChunks[0])\n",
    "features_0 = last_hidden_states_0[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3faba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states_1 = get_hidden_states(paddedChunks[1], attentionChunks[1])\n",
    "features_1 = last_hidden_states_1[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3beabd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states_2 = get_hidden_states(paddedChunks[2], attentionChunks[2])\n",
    "features_2 = last_hidden_states_2[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e272a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states_3 = get_hidden_states(paddedChunks[3], attentionChunks[3])\n",
    "features_3 = last_hidden_states_3[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "feece4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states_4 = get_hidden_states(paddedChunks[4], attentionChunks[4])\n",
    "features_4 = last_hidden_states_4[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c5afba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = np.array_split( np.array( list( df['label'] ) ), 5 )\n",
    "train_Y_0 = train_Y[0]\n",
    "train_Y_1 = train_Y[1]\n",
    "train_Y_2 = train_Y[2]\n",
    "train_Y_3 = train_Y[3]\n",
    "train_Y_4 = train_Y[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88ecdbfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(features_0, train_Y_0)\n",
    "lr_clf.fit(features_1, train_Y_1)\n",
    "lr_clf.fit(features_2, train_Y_2)\n",
    "lr_clf.fit(features_3, train_Y_3)\n",
    "lr_clf.fit(features_4, train_Y_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e3e7a",
   "metadata": {},
   "source": [
    "## Running with Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2bcbdf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "159 76\n",
      "\n",
      "[101, 1996, 2143, 2022, 11633, 2000, 2054, 2028, 3653, 23545, 2022, 1996, 2338, 1055, 5519, 18458, 2008, 1011, 4013, 2078, 1011, 2468, 2040, 1011, 4013, 2078, 1011, 2022, 2006, 1996, 2067, 1997, 1011, 4013, 2078, 1011, 6687, 2021, 1011, 4013, 2078, 1011, 2031, 2053, 2801, 2040, 1011, 4013, 2078, 1011, 2022, 2012, 1011, 4013, 2078, 1011, 2287, 1998, 2008, 2051, 2022, 1037, 25085, 1998, 9062, 19502, 2053, 3043, 2129, 2214, 1011, 4013, 2078, 1011, 2022, 102]\n",
      "(2210, 71)\n",
      "(2210, 71)\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = test_df['lemmatized'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "test_padded = np.array( [i + [0]*(max_len-len(i)) for i in test_tokenized.values] )\n",
    "\n",
    "#AHHHH, The longest in Test Padded is Longer than in the Training set! \n",
    "#This caused test_padded to be an array of lists instead of 2d array\n",
    "lens = [len(i) for i in test_padded]\n",
    "print(all([i == 71 for i in lens]))\n",
    "for i, length in enumerate(lens):\n",
    "    if length != 71:\n",
    "        print(i, length)\n",
    "        print()\n",
    "        print(test_padded[i])\n",
    "\n",
    "#Cut off to len 71 only.\n",
    "test_padded = np.array( [np.array( i[:max_len] ) for i in test_padded] )\n",
    "\n",
    "test_attention_mask = np.where(test_padded != 0, 1, 0)\n",
    "print( np.array(test_padded).shape )\n",
    "print( np.array(test_attention_mask).shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa6420fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = torch.tensor(test_padded)\n",
    "test_input_ids = test_input_ids.to(test_input_ids.device).long() \n",
    "test_attention_mask = torch.tensor(test_attention_mask)\n",
    "test_attention_mask = test_attention_mask.to(test_attention_mask.device).long()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_hidden_states = model(test_input_ids, attention_mask=test_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2fe5a2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2, ..., 2, 2, 1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = test_hidden_states[0][:,0,:].numpy()\n",
    "test_predict_Y = lr_clf.predict(test_features)\n",
    "test_predict_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "260e017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_Y = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "68d25257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x19d999650d0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "MulticlassCM = confusion_matrix( true_Y, test_predict_Y )\n",
    "MultiClassDisp = ConfusionMatrixDisplay( MulticlassCM )\n",
    "MultiClassDisp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b5806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
