{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e811c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import gensim.parsing.preprocessing as gen_preproc\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481aeda6",
   "metadata": {},
   "source": [
    "# Data Loading and Cleanup\n",
    "Identical to what occured in the Initial Models file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f87cbad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>basicProc</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>the rock is destined to be the 21st century s ...</td>\n",
       "      <td>the rock is destin to be the 21st centuri s ne...</td>\n",
       "      <td>the rock be destine to be the 21st century s n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "      <td>the gorgeous elabor continu of the lord of the...</td>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "      <td>singer composer bryan adams contributes a slew...</td>\n",
       "      <td>singer compos bryan adam contribut a slew of s...</td>\n",
       "      <td>singer composer bryan adams contribute a slew ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>you d think by now america would have had enou...</td>\n",
       "      <td>you d think by now america would have had enou...</td>\n",
       "      <td>-PRON- d think by now america would have have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>yet the act is still charming here</td>\n",
       "      <td>yet the act is still charm here</td>\n",
       "      <td>yet the act be still charm here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      4  The Rock is destined to be the 21st Century 's...   \n",
       "1      5  The gorgeously elaborate continuation of `` Th...   \n",
       "2      4  Singer/composer Bryan Adams contributes a slew...   \n",
       "3      3  You 'd think by now America would have had eno...   \n",
       "4      4               Yet the act is still charming here .   \n",
       "\n",
       "                                           basicProc  \\\n",
       "0  the rock is destined to be the 21st century s ...   \n",
       "1  the gorgeously elaborate continuation of the l...   \n",
       "2  singer composer bryan adams contributes a slew...   \n",
       "3  you d think by now america would have had enou...   \n",
       "4                 yet the act is still charming here   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  the rock is destin to be the 21st centuri s ne...   \n",
       "1  the gorgeous elabor continu of the lord of the...   \n",
       "2  singer compos bryan adam contribut a slew of s...   \n",
       "3  you d think by now america would have had enou...   \n",
       "4                    yet the act is still charm here   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  the rock be destine to be the 21st century s n...  \n",
       "1  the gorgeously elaborate continuation of the l...  \n",
       "2  singer composer bryan adams contribute a slew ...  \n",
       "3  -PRON- d think by now america would have have ...  \n",
       "4                    yet the act be still charm here  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv( 'parsed_train.txt', sep='\\t', header=None,\n",
    "                   names=['label', 'text'] )\n",
    "df['label'] = df['label'].apply(lambda x: int( x.replace(\"__label__\", \"\") ) )\n",
    "\n",
    "basicPreproc = [lambda x: x.lower(), gen_preproc.strip_tags, \\\n",
    "    gen_preproc.strip_punctuation, gen_preproc.strip_non_alphanum, \\\n",
    "    gen_preproc.strip_multiple_whitespaces] #, gen_preproc.strip_short, gen_preproc.remove_stopwords\n",
    "\n",
    "df['basicProc'] = df['text'].apply(lambda x: ' '.join( gen_preproc.preprocess_string(x, basicPreproc)))\n",
    "df['stemmed'] = df['basicProc'].apply(lambda x: gen_preproc.stem_text(x))\n",
    "df['lemmatized'] = df['basicProc'].apply(lambda x: ' '.join( [token.lemma_ for token in nlp(x)] ) )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fb55ca",
   "metadata": {},
   "source": [
    "Do the same but for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09bcad56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>basicProc</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>effective but too tepid biopic</td>\n",
       "      <td>effect but too tepid biopic</td>\n",
       "      <td>effective but too tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>if you sometim like to go to the movi to have ...</td>\n",
       "      <td>if -PRON- sometimes like to go to the movie to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>emerges as something rare an issue movie that ...</td>\n",
       "      <td>emerg as someth rare an issu movi that s so ho...</td>\n",
       "      <td>emerge as something rare an issue movie that s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The film provides some great insight into the ...</td>\n",
       "      <td>the film provides some great insight into the ...</td>\n",
       "      <td>the film provid some great insight into the ne...</td>\n",
       "      <td>the film provide some great insight into the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>offers that rare combination of entertainment ...</td>\n",
       "      <td>offer that rare combin of entertain and educ</td>\n",
       "      <td>offer that rare combination of entertainment a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      3                     Effective but too-tepid biopic   \n",
       "1      4  If you sometimes like to go to the movies to h...   \n",
       "2      5  Emerges as something rare , an issue movie tha...   \n",
       "3      3  The film provides some great insight into the ...   \n",
       "4      5  Offers that rare combination of entertainment ...   \n",
       "\n",
       "                                           basicProc  \\\n",
       "0                     effective but too tepid biopic   \n",
       "1  if you sometimes like to go to the movies to h...   \n",
       "2  emerges as something rare an issue movie that ...   \n",
       "3  the film provides some great insight into the ...   \n",
       "4  offers that rare combination of entertainment ...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0                        effect but too tepid biopic   \n",
       "1  if you sometim like to go to the movi to have ...   \n",
       "2  emerg as someth rare an issu movi that s so ho...   \n",
       "3  the film provid some great insight into the ne...   \n",
       "4       offer that rare combin of entertain and educ   \n",
       "\n",
       "                                          lemmatized  \n",
       "0                     effective but too tepid biopic  \n",
       "1  if -PRON- sometimes like to go to the movie to...  \n",
       "2  emerge as something rare an issue movie that s...  \n",
       "3  the film provide some great insight into the n...  \n",
       "4  offer that rare combination of entertainment a...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv( 'parsed_test.txt', sep='\\t', header=None,\n",
    "                   names=['label', 'text'] )\n",
    "test_df['label'] = test_df['label'].apply(lambda x: int( x.replace(\"__label__\", \"\") ) )\n",
    "test_df['basicProc'] = test_df['text'].apply(lambda x: ' '.join( gen_preproc.preprocess_string(x, basicPreproc)))\n",
    "test_df['stemmed'] = test_df['basicProc'].apply(lambda x: gen_preproc.stem_text(x))\n",
    "test_df['lemmatized'] = test_df['basicProc'].apply(lambda x: ' '.join( [token.lemma_ for token in nlp(x)] ) )\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8a519f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>basicProc</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>the rock is destined to be the 21st century s ...</td>\n",
       "      <td>the rock is destin to be the 21st centuri s ne...</td>\n",
       "      <td>the rock be destine to be the 21st century s n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "      <td>the gorgeous elabor continu of the lord of the...</td>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "      <td>singer composer bryan adams contributes a slew...</td>\n",
       "      <td>singer compos bryan adam contribut a slew of s...</td>\n",
       "      <td>singer composer bryan adams contribute a slew ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>you d think by now america would have had enou...</td>\n",
       "      <td>you d think by now america would have had enou...</td>\n",
       "      <td>-PRON- d think by now america would have have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>yet the act is still charming here</td>\n",
       "      <td>yet the act is still charm here</td>\n",
       "      <td>yet the act be still charm here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0    4.0  The Rock is destined to be the 21st Century 's...   \n",
       "1    5.0  The gorgeously elaborate continuation of `` Th...   \n",
       "2    4.0  Singer/composer Bryan Adams contributes a slew...   \n",
       "3    3.0  You 'd think by now America would have had eno...   \n",
       "4    4.0               Yet the act is still charming here .   \n",
       "\n",
       "                                           basicProc  \\\n",
       "0  the rock is destined to be the 21st century s ...   \n",
       "1  the gorgeously elaborate continuation of the l...   \n",
       "2  singer composer bryan adams contributes a slew...   \n",
       "3  you d think by now america would have had enou...   \n",
       "4                 yet the act is still charming here   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  the rock is destin to be the 21st centuri s ne...   \n",
       "1  the gorgeous elabor continu of the lord of the...   \n",
       "2  singer compos bryan adam contribut a slew of s...   \n",
       "3  you d think by now america would have had enou...   \n",
       "4                    yet the act is still charm here   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  the rock be destine to be the 21st century s n...  \n",
       "1  the gorgeously elaborate continuation of the l...  \n",
       "2  singer composer bryan adams contribute a slew ...  \n",
       "3  -PRON- d think by now america would have have ...  \n",
       "4                    yet the act be still charm here  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].apply(lambda x: float(x))\n",
    "test_df['label'] = test_df['label'].apply(lambda x: float(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579311e4",
   "metadata": {},
   "source": [
    "## Setup Tokenization for input into LSTM\n",
    "\n",
    "Like other networks, an LSTM takes inputs of vectors. Therefore we do need to turn our input text back into vectors. However, we will be feeding in each word at a time, so its not necessary to do anything too complicated. \n",
    "\n",
    "#### Questions:\n",
    "- How are ngrams used in an LSTM?\n",
    "- Is there any benefit to feeding in something like a w2v vector instead of a one-hot encoded vector? One hot obviously relies on a consistant vocab, which fasttext could get around. \n",
    "- Why would you want to stack multiple levels of LSTMs on top of eachother?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d5d0be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxNumWords = 5000 #Most frequent\n",
    "maxSeqLeng = 45 #See Initial Models file for reviews length distro justifying this. \n",
    "embedDim = 100\n",
    "epochs = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4bed47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12724 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer( num_words=maxNumWords ) #already did other preproc, but could include more here\n",
    "tokenizer.fit_on_texts(df['lemmatized'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4485478c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input data tensor: (8544, 45)\n",
      "Vector 0: \n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    1  490    6 3229    7    6    1 2701  893    8   93    4\n",
      "    9    2    8   69    7   25    3 2339   61  109   34 1880 2702 2703\n",
      " 1671   40  894]\n",
      "Vector 1: \n",
      " [   0    0    0    0    0    0    0    0    0    0    1 2704 1881    5\n",
      "    1 3230    5    1  938    6   37  895    9    3    5  358   60   24\n",
      " 3231 1129  669  210   67 1065  896    8 2340  616    5 1294 1672 1672\n",
      "    8  446 1295]\n",
      "\n",
      "Line 0: \n",
      " the rock be destine to be the 21st century s new conan and that -PRON- s go to make a splash even great than arnold schwarzenegger jean claud van damme or steven segal\n",
      "Line 1: \n",
      " the gorgeously elaborate continuation of the lord of the ring trilogy be so huge that a column of word can not adequately describe co writer director peter jackson s expand vision of j r r tolkien s middle earth\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences( df['lemmatized'].values )\n",
    "X = pad_sequences( X, maxlen=maxSeqLeng) #Adds 0's in front of my sentances that are shorter than maxSeqLen\n",
    "Y = np.array( list( df['label'] ) )\n",
    "\n",
    "print('Shape of Input data tensor:', X.shape)\n",
    "print( \"Vector 0: \\n\", X[0] )\n",
    "print( \"Vector 1: \\n\", X[1] )\n",
    "print() \n",
    "print( \"Line 0: \\n\", df['lemmatized'][0] )\n",
    "print( \"Line 1: \\n\", df['lemmatized'][1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67764b6e",
   "metadata": {},
   "source": [
    "We can see that our X array encodes each term in the sentance as a value (1 - 5000) with the most common words having lower numbers (eg \"the\" is represented by a 1). The order of the vector is the order which terms appear in the sequence, eg [1, 1] means the word \"the\" occurs, then the word \"the\" would occur again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80c7659",
   "metadata": {},
   "source": [
    "## LSTM Model Setup\n",
    "\n",
    "We want to have multiple layers that do different things. \n",
    "- First we embed our text into vectors \n",
    "- Then we setup our LSTM network \n",
    "- The LSTM outputs are naturally the same size as the historical data it keeps - eg the same size as the input; so we add a dense layer that transforms our historical information into the info we care about - 1 predicted label. \n",
    "- Of course, we need to decide how we're evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34b76e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9e03fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 45, 100)           500000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 45, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 580,501\n",
      "Trainable params: 580,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 45, 100)           500000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 45, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 580,501\n",
      "Trainable params: 580,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(maxNumWords, embedDim, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(embedDim, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='mean_squared_error') #Treating this is a regression problem, so lets use MSE\n",
    "print(model.summary())\n",
    "\n",
    "modelF1 = Sequential()\n",
    "modelF1.add(Embedding(maxNumWords, embedDim, input_length=X.shape[1]))\n",
    "modelF1.add(SpatialDropout1D(0.2))\n",
    "modelF1.add(LSTM(embedDim, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelF1.add(Dense(1, activation='softmax'))\n",
    "#modelF1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[get_f1])\n",
    "modelF1.compile(optimizer='adam',loss='mean_squared_error', metrics=[get_f1])\n",
    "print(modelF1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55f65b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "121/121 [==============================] - 10s 80ms/step - loss: 6.1673 - val_loss: 3.2749\n",
      "Epoch 2/100\n",
      "121/121 [==============================] - 9s 78ms/step - loss: 6.1673 - val_loss: 3.2749\n",
      "Epoch 3/100\n",
      "121/121 [==============================] - 10s 83ms/step - loss: 6.1673 - val_loss: 3.2749\n",
      "Epoch 4/100\n",
      "121/121 [==============================] - 9s 75ms/step - loss: 6.1673 - val_loss: 3.2749\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, Y, epochs=epochs, batch_size=batch_size, \\\n",
    "            validation_split=0.1, callbacks=[EarlyStopping( \\\n",
    "                               monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16180a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "121/121 [==============================] - 10s 87ms/step - loss: 6.1673 - get_f1: 1.0000 - val_loss: 3.2749 - val_get_f1: 1.0000\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 10s 85ms/step - loss: 6.1673 - get_f1: 1.0000 - val_loss: 3.2749 - val_get_f1: 1.0000\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 11s 91ms/step - loss: 6.1673 - get_f1: 1.0000 - val_loss: 3.2749 - val_get_f1: 1.0000\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 11s 88ms/step - loss: 6.1673 - get_f1: 1.0000 - val_loss: 3.2749 - val_get_f1: 1.0000\n"
     ]
    }
   ],
   "source": [
    "historyF1 = modelF1.fit(X, Y, epochs=epochs, batch_size=batch_size, \\\n",
    "            validation_split=0.1, callbacks=[EarlyStopping( \\\n",
    "                               monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f68ca6",
   "metadata": {},
   "source": [
    "### Well those didn't work! \n",
    "Why though? Its possible that my loss was way too high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "288fd17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 45, 100)           500000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_13 (Spatia (None, 45, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 580,501\n",
      "Trainable params: 580,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "modelLr = Sequential()\n",
    "modelLr.add(Embedding(maxNumWords, embedDim, input_length=X.shape[1]))\n",
    "modelLr.add(SpatialDropout1D(0.2))\n",
    "modelLr.add(LSTM(embedDim, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelLr.add(Dense(1, activation='softmax'))\n",
    "modelLr.compile(optimizer=opt,loss='mean_squared_error') \n",
    "print(modelLr.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72c95a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "121/121 [==============================] - 12s 97ms/step - loss: 6.1673 - val_loss: 3.2749\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 11s 93ms/step - loss: 6.1673 - val_loss: 3.2749\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 11s 90ms/step - loss: 6.1673 - val_loss: 3.2749\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 10s 86ms/step - loss: 6.1673 - val_loss: 3.2749\n"
     ]
    }
   ],
   "source": [
    "history = modelLr.fit(X, Y, epochs=epochs, batch_size=batch_size, \\\n",
    "            validation_split=0.1, callbacks=[EarlyStopping( \\\n",
    "                               monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2536fb8",
   "metadata": {},
   "source": [
    "#### Nope, that wasn't it! So lets try adjusting my inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eae9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2vModel = Word2Vec( df['tokenized'], vector_size=500, window=5, min_count=3, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9dfaa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 45, 500)           2500000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_15 (Spatia (None, 45, 500)           0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 500)               2002000   \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 4,502,501\n",
      "Trainable params: 4,502,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "embedDim = 500 #Makes things super slow.\n",
    "\n",
    "model500 = Sequential()\n",
    "model500.add(Embedding(maxNumWords, embedDim, input_length=X.shape[1]))\n",
    "model500.add(SpatialDropout1D(0.2))\n",
    "model500.add(LSTM(embedDim, dropout=0.2, recurrent_dropout=0.2))\n",
    "model500.add(Dense(1, activation='softmax'))\n",
    "model500.compile(optimizer=opt,loss='mean_squared_error', metrics=['mean_squared_error']) \n",
    "print(model500.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4385114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "121/121 [==============================] - 131s 1s/step - loss: 6.1673 - mean_squared_error: 6.1673 - val_loss: 3.2749 - val_mean_squared_error: 3.2749\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 140s 1s/step - loss: 6.1673 - mean_squared_error: 6.1673 - val_loss: 3.2749 - val_mean_squared_error: 3.2749\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 150s 1s/step - loss: 6.1673 - mean_squared_error: 6.1673 - val_loss: 3.2749 - val_mean_squared_error: 3.2749\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 162s 1s/step - loss: 6.1673 - mean_squared_error: 6.1673 - val_loss: 3.2749 - val_mean_squared_error: 3.2749\n"
     ]
    }
   ],
   "source": [
    "history = model500.fit(X, Y, epochs=epochs, batch_size=batch_size, \\\n",
    "            validation_split=0.1, callbacks=[EarlyStopping( \\\n",
    "                               monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad5adb1",
   "metadata": {},
   "source": [
    "### Not totally sure why it won't change. \n",
    "Not embedding dimension, not Learning rate. Lets try to make it a classification problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84b2aabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 4]\n",
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "print(Y[:3])\n",
    "YMultiClass = to_categorical(Y)\n",
    "print(YMultiClass[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f41c5572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 45, 100)           500000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_17 (Spatia (None, 45, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 581,006\n",
      "Trainable params: 581,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "embedDim = 100\n",
    "\n",
    "modelMC = Sequential()\n",
    "modelMC.add(Embedding(maxNumWords, embedDim, input_length=X.shape[1]))\n",
    "modelMC.add(SpatialDropout1D(0.2))\n",
    "modelMC.add(LSTM(embedDim, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelMC.add(Dense(6, activation='softmax'))\n",
    "modelMC.compile(optimizer=opt,loss='mean_squared_error', metrics=['Accuracy']) \n",
    "print(modelMC.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "296a42d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "121/121 [==============================] - 11s 89ms/step - loss: 0.1348 - accuracy: 0.0000e+00 - val_loss: 0.1356 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 10s 86ms/step - loss: 0.1303 - accuracy: 0.0000e+00 - val_loss: 0.1361 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 11s 88ms/step - loss: 0.1300 - accuracy: 0.0000e+00 - val_loss: 0.1344 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 11s 92ms/step - loss: 0.1297 - accuracy: 0.0000e+00 - val_loss: 0.1329 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "121/121 [==============================] - 11s 89ms/step - loss: 0.1294 - accuracy: 0.0000e+00 - val_loss: 0.1349 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "121/121 [==============================] - 16s 135ms/step - loss: 0.1285 - accuracy: 0.0000e+00 - val_loss: 0.1333 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "121/121 [==============================] - 17s 139ms/step - loss: 0.1271 - accuracy: 0.0000e+00 - val_loss: 0.1332 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = modelMC.fit(X, YMultiClass, epochs=epochs, batch_size=batch_size, \\\n",
    "            validation_split=0.1, callbacks=[EarlyStopping( \\\n",
    "                               monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c64310c",
   "metadata": {},
   "source": [
    "Well, at least loss is decreasing. However, accuracy is at 0. This is because I'M STUPID AND ITS SOFTMAX OUTPUT. Softmax will output a continuous value. However, mean_square_error is still not what you should be doing for ~multiclass problems. So lets use categorical cross entropy instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "01f3b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 45, 100)           500000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_18 (Spatia (None, 45, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 581,006\n",
      "Trainable params: 581,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "embedDim = 100\n",
    "\n",
    "modelMCCrossEntropy = Sequential()\n",
    "modelMCCrossEntropy.add(Embedding(maxNumWords, embedDim, input_length=X.shape[1]))\n",
    "modelMCCrossEntropy.add(SpatialDropout1D(0.2))\n",
    "modelMCCrossEntropy.add(LSTM(embedDim, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelMCCrossEntropy.add(Dense(6, activation='softmax'))\n",
    "modelMCCrossEntropy.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['Accuracy']) \n",
    "print(modelMCCrossEntropy.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1381de90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "121/121 [==============================] - 11s 93ms/step - loss: 1.6844 - accuracy: 0.0000e+00 - val_loss: 1.6697 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 11s 93ms/step - loss: 1.5644 - accuracy: 0.0000e+00 - val_loss: 1.6393 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 11s 93ms/step - loss: 1.5615 - accuracy: 0.0000e+00 - val_loss: 1.6619 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 11s 94ms/step - loss: 1.5575 - accuracy: 0.0000e+00 - val_loss: 1.6263 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "121/121 [==============================] - 12s 99ms/step - loss: 1.5532 - accuracy: 0.0000e+00 - val_loss: 1.6259 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "121/121 [==============================] - 11s 92ms/step - loss: 1.5447 - accuracy: 0.0000e+00 - val_loss: 1.6523 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "121/121 [==============================] - 12s 97ms/step - loss: 1.5252 - accuracy: 0.0000e+00 - val_loss: 1.6102 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "121/121 [==============================] - 14s 114ms/step - loss: 1.4669 - accuracy: 0.0000e+00 - val_loss: 1.5847 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "121/121 [==============================] - 20s 164ms/step - loss: 1.3822 - accuracy: 0.0000e+00 - val_loss: 1.4439 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "121/121 [==============================] - 13s 107ms/step - loss: 1.3108 - accuracy: 0.0000e+00 - val_loss: 1.4486 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = modelMCCrossEntropy.fit(X, YMultiClass, epochs=epochs, batch_size=batch_size, \\\n",
    "            validation_split=0.1, callbacks=[EarlyStopping( \\\n",
    "                               monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b743097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 45, 100)           500000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 45, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 581,006\n",
      "Trainable params: 581,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.0003) \n",
    "#Now that loss is going down, add a higher LR & make sure we have more epochs. \n",
    "embedDim = 100\n",
    "epochs = 100\n",
    "\n",
    "modelMCCrossEntropyFull = Sequential()\n",
    "modelMCCrossEntropyFull.add(Embedding(maxNumWords, embedDim, input_length=X.shape[1]))\n",
    "modelMCCrossEntropyFull.add(SpatialDropout1D(0.2))\n",
    "modelMCCrossEntropyFull.add(LSTM(embedDim, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelMCCrossEntropyFull.add(Dense(6, activation='softmax'))\n",
    "modelMCCrossEntropyFull.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['mean_squared_error']) \n",
    "print(modelMCCrossEntropyFull.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ec0cdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "121/121 [==============================] - 10s 86ms/step - loss: 1.6217 - mean_squared_error: 0.1322 - val_loss: 1.6453 - val_mean_squared_error: 0.1360\n",
      "Epoch 2/100\n",
      "121/121 [==============================] - 10s 84ms/step - loss: 1.5610 - mean_squared_error: 0.1300 - val_loss: 1.6385 - val_mean_squared_error: 0.1344\n",
      "Epoch 3/100\n",
      "121/121 [==============================] - 10s 84ms/step - loss: 1.5362 - mean_squared_error: 0.1283 - val_loss: 1.5783 - val_mean_squared_error: 0.1308\n",
      "Epoch 4/100\n",
      "121/121 [==============================] - 9s 78ms/step - loss: 1.4155 - mean_squared_error: 0.1202 - val_loss: 1.5274 - val_mean_squared_error: 0.1281\n",
      "Epoch 5/100\n",
      "121/121 [==============================] - 10s 79ms/step - loss: 1.2540 - mean_squared_error: 0.1095 - val_loss: 1.4204 - val_mean_squared_error: 0.1227\n",
      "Epoch 6/100\n",
      "121/121 [==============================] - 13s 104ms/step - loss: 1.1130 - mean_squared_error: 0.0993 - val_loss: 1.4234 - val_mean_squared_error: 0.1217\n",
      "Epoch 7/100\n",
      "121/121 [==============================] - 10s 84ms/step - loss: 1.0104 - mean_squared_error: 0.0911 - val_loss: 1.4944 - val_mean_squared_error: 0.1267\n",
      "Epoch 8/100\n",
      "121/121 [==============================] - 11s 90ms/step - loss: 0.9257 - mean_squared_error: 0.0838 - val_loss: 1.6463 - val_mean_squared_error: 0.1326\n"
     ]
    }
   ],
   "source": [
    "history = modelMCCrossEntropyFull.fit(X, YMultiClass, epochs=epochs, batch_size=batch_size, \\\n",
    "            validation_split=0.1, callbacks=[EarlyStopping( \\\n",
    "                               monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c26bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = tokenizer.texts_to_sequences( test_df['lemmatized'].values )\n",
    "test_X = pad_sequences( test_X, maxlen=maxSeqLeng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f28434e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_Y = modelMCCrossEntropyFull.predict( test_X )\n",
    "predict_Y = np.argmax(predict_Y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b05a572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 5, ..., 5, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f698c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label'] = test_df['label'].apply(lambda x: int(x))\n",
    "true_Y = list( test_df['label'] )\n",
    "true_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f017f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1e47a617730>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "MulticlassCM = confusion_matrix( true_Y, predict_Y )\n",
    "MultiClassDisp = ConfusionMatrixDisplay( MulticlassCM )\n",
    "MultiClassDisp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf21cbc",
   "metadata": {},
   "source": [
    "### Use the regression version of the model\n",
    "Did the regression model loss not change because it just converged very very quickly in all versions of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "672339a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_Y_reg = model.predict( test_X )\n",
    "predict_Y_reg = np.argmax(predict_Y_reg, axis=1)\n",
    "print(max(predict_Y_reg))\n",
    "predict_Y_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acac07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
