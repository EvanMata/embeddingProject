{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "94cba226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.parsing.preprocessing as gen_preproc\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0b8e5",
   "metadata": {},
   "source": [
    "## Data Preview & Basic Cleanup\n",
    "\n",
    "Simple open the file and preview the basic data in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "af6b0567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__4</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__5</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__4</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__3</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__4</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text\n",
       "0  __label__4  The Rock is destined to be the 21st Century 's...\n",
       "1  __label__5  The gorgeously elaborate continuation of `` Th...\n",
       "2  __label__4  Singer/composer Bryan Adams contributes a slew...\n",
       "3  __label__3  You 'd think by now America would have had eno...\n",
       "4  __label__4               Yet the act is still charming here ."
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv( 'parsed_train.txt', sep='\\t', header=None,\n",
    "                   names=['label', 'text'] )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94932737",
   "metadata": {},
   "source": [
    "Do some basic cleanup of labels, not yet preprocessing the text at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3559c6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      4  The Rock is destined to be the 21st Century 's...\n",
       "1      5  The gorgeously elaborate continuation of `` Th...\n",
       "2      4  Singer/composer Bryan Adams contributes a slew...\n",
       "3      3  You 'd think by now America would have had eno...\n",
       "4      4               Yet the act is still charming here ."
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].apply(lambda x: int( x.replace(\"__label__\", \"\") ) )\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00ac2b0",
   "metadata": {},
   "source": [
    "#### Are my labels balanced and evenly distributed? \n",
    "\n",
    "Or do I need to pay special care to my input data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c184ca6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1092., 2218., 1624., 2322., 1288.]),\n",
       " array([1, 2, 3, 4, 5, 6]),\n",
       " <BarContainer object of 5 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['label'], bins=[1,2,3,4,5,6], align='left', rwidth=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf9516",
   "metadata": {},
   "source": [
    "A bit unbalanced, but not too much. Also a symetrical distrobution, not many extra positive or negative reviews; though theres slight more positive items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf353e18",
   "metadata": {},
   "source": [
    "#### How many reviews are more than a single sentence?\n",
    "\n",
    "When getting to embedding models, can I treat each item as a single sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e32328d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3555d874",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Time to do a bit of preprocessing for different items. All items will be lowercase, punctuation & stop words stripped as well as some other basic preprocessing; plus one column thats stemmed and another lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6912153e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>basicProc</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>the rock is destined to be the 21st century s ...</td>\n",
       "      <td>the rock is destin to be the 21st centuri s ne...</td>\n",
       "      <td>the rock be destine to be the 21st century s n...</td>\n",
       "      <td>[the, rock, destine, the, 21st, century, new, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "      <td>the gorgeous elabor continu of the lord of the...</td>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "      <td>[the, gorgeously, elaborate, continuation, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "      <td>singer composer bryan adams contributes a slew...</td>\n",
       "      <td>singer compos bryan adam contribut a slew of s...</td>\n",
       "      <td>singer composer bryan adams contribute a slew ...</td>\n",
       "      <td>[singer, composer, bryan, adams, contribute, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>you d think by now america would have had enou...</td>\n",
       "      <td>you d think by now america would have had enou...</td>\n",
       "      <td>-PRON- d think by now america would have have ...</td>\n",
       "      <td>[-PRON-, think, now, america, would, have, hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>yet the act is still charming here</td>\n",
       "      <td>yet the act is still charm here</td>\n",
       "      <td>yet the act be still charm here</td>\n",
       "      <td>[yet, the, act, still, charm, here]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      4  The Rock is destined to be the 21st Century 's...   \n",
       "1      5  The gorgeously elaborate continuation of `` Th...   \n",
       "2      4  Singer/composer Bryan Adams contributes a slew...   \n",
       "3      3  You 'd think by now America would have had eno...   \n",
       "4      4               Yet the act is still charming here .   \n",
       "\n",
       "                                           basicProc  \\\n",
       "0  the rock is destined to be the 21st century s ...   \n",
       "1  the gorgeously elaborate continuation of the l...   \n",
       "2  singer composer bryan adams contributes a slew...   \n",
       "3  you d think by now america would have had enou...   \n",
       "4                 yet the act is still charming here   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  the rock is destin to be the 21st centuri s ne...   \n",
       "1  the gorgeous elabor continu of the lord of the...   \n",
       "2  singer compos bryan adam contribut a slew of s...   \n",
       "3  you d think by now america would have had enou...   \n",
       "4                    yet the act is still charm here   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  the rock be destine to be the 21st century s n...   \n",
       "1  the gorgeously elaborate continuation of the l...   \n",
       "2  singer composer bryan adams contribute a slew ...   \n",
       "3  -PRON- d think by now america would have have ...   \n",
       "4                    yet the act be still charm here   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [the, rock, destine, the, 21st, century, new, ...  \n",
       "1  [the, gorgeously, elaborate, continuation, the...  \n",
       "2  [singer, composer, bryan, adams, contribute, s...  \n",
       "3  [-PRON-, think, now, america, would, have, hav...  \n",
       "4                [yet, the, act, still, charm, here]  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basicPreproc = [lambda x: x.lower(), gen_preproc.strip_tags, \\\n",
    "    gen_preproc.strip_punctuation, gen_preproc.strip_non_alphanum, \\\n",
    "    gen_preproc.strip_multiple_whitespaces] #, gen_preproc.strip_short, gen_preproc.remove_stopwords\n",
    "\n",
    "df['basicProc'] = df['text'].apply(lambda x: ' '.join( gen_preproc.preprocess_string(x, basicPreproc)))\n",
    "df['stemmed'] = df['basicProc'].apply(lambda x: gen_preproc.stem_text(x))\n",
    "df['lemmatized'] = df['basicProc'].apply(lambda x: ' '.join( [token.lemma_ for token in nlp(x)] ) )\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd6c2b",
   "metadata": {},
   "source": [
    "## Creating BOW-Esk Input Vectors & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f27df76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bowVec = CountVectorizer(binary=True, ngram_range=(1,2), min_df=2, max_features=5000)\n",
    "cvVec = CountVectorizer(binary=False, ngram_range=(1,2), min_df=2, max_features=5000)\n",
    "tfidfVec = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=5000)\n",
    "\n",
    "bowDTM = bowVec.fit_transform(df['lemmatized'])\n",
    "cvDTM = cvVec.fit_transform(df['lemmatized'])\n",
    "tfidfDTM = tfidfVec.fit_transform(df['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c2b175f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowModel = svm.SVR()\n",
    "cvModel = svm.SVR()\n",
    "tfidfModel = svm.SVR()\n",
    "\n",
    "bowModel.fit(bowDTM, df['label'])\n",
    "cvModel.fit(cvDTM, df['label'])\n",
    "tfidfModel.fit(tfidfDTM, df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec340d0f",
   "metadata": {},
   "source": [
    "## Load in the Test Dataset And Parse the same way as Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "91d5b269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>basicProc</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>effective but too tepid biopic</td>\n",
       "      <td>effect but too tepid biopic</td>\n",
       "      <td>effective but too tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>if you sometim like to go to the movi to have ...</td>\n",
       "      <td>if -PRON- sometimes like to go to the movie to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>emerges as something rare an issue movie that ...</td>\n",
       "      <td>emerg as someth rare an issu movi that s so ho...</td>\n",
       "      <td>emerge as something rare an issue movie that s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The film provides some great insight into the ...</td>\n",
       "      <td>the film provides some great insight into the ...</td>\n",
       "      <td>the film provid some great insight into the ne...</td>\n",
       "      <td>the film provide some great insight into the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>offers that rare combination of entertainment ...</td>\n",
       "      <td>offer that rare combin of entertain and educ</td>\n",
       "      <td>offer that rare combination of entertainment a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      3                     Effective but too-tepid biopic   \n",
       "1      4  If you sometimes like to go to the movies to h...   \n",
       "2      5  Emerges as something rare , an issue movie tha...   \n",
       "3      3  The film provides some great insight into the ...   \n",
       "4      5  Offers that rare combination of entertainment ...   \n",
       "\n",
       "                                           basicProc  \\\n",
       "0                     effective but too tepid biopic   \n",
       "1  if you sometimes like to go to the movies to h...   \n",
       "2  emerges as something rare an issue movie that ...   \n",
       "3  the film provides some great insight into the ...   \n",
       "4  offers that rare combination of entertainment ...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0                        effect but too tepid biopic   \n",
       "1  if you sometim like to go to the movi to have ...   \n",
       "2  emerg as someth rare an issu movi that s so ho...   \n",
       "3  the film provid some great insight into the ne...   \n",
       "4       offer that rare combin of entertain and educ   \n",
       "\n",
       "                                          lemmatized  \n",
       "0                     effective but too tepid biopic  \n",
       "1  if -PRON- sometimes like to go to the movie to...  \n",
       "2  emerge as something rare an issue movie that s...  \n",
       "3  the film provide some great insight into the n...  \n",
       "4  offer that rare combination of entertainment a...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv( 'parsed_test.txt', sep='\\t', header=None,\n",
    "                   names=['label', 'text'] )\n",
    "test_df['label'] = test_df['label'].apply(lambda x: int( x.replace(\"__label__\", \"\") ) )\n",
    "test_df['basicProc'] = test_df['text'].apply(lambda x: ' '.join( gen_preproc.preprocess_string(x, basicPreproc)))\n",
    "test_df['stemmed'] = test_df['basicProc'].apply(lambda x: gen_preproc.stem_text(x))\n",
    "test_df['lemmatized'] = test_df['basicProc'].apply(lambda x: ' '.join( [token.lemma_ for token in nlp(x)] ) )\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149e20c",
   "metadata": {},
   "source": [
    "## BOW - Esk Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "77907f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testBowDTM = bowVec.transform( test_df['lemmatized'] )\n",
    "testCvDTM = cvVec.transform( test_df['lemmatized'] )\n",
    "testTfidfDTM = tfidfVec.transform( test_df['lemmatized'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ece44f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow R^2:  0.33200996559026075\n",
      "Cv R^2:  0.3229852810198336\n",
      "Tfidf R^2:  0.3627280831959865\n"
     ]
    }
   ],
   "source": [
    "print( 'Bow R^2: ',  bowModel.score(testBowDTM, test_df['label']) )\n",
    "print( 'Cv R^2: ',  cvModel.score(testCvDTM, test_df['label'])  )\n",
    "print( 'Tfidf R^2: ',  tfidfModel.score(testTfidfDTM, test_df['label'])  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dbfded",
   "metadata": {},
   "source": [
    "Fairly Comparable results. Probably impacted by the fact that each text is relatively short. We didn't do any analysis of our actual vectorizer's here; technically we did keep max 5k vocab words but we don't know if we even hit that many. We don't know how sparse our input vectors are either. We could almost certainly improve these results (and maybe show more distinction between items) if we made our vectorizers more dense using something like PCA (principle component analysis) or SVD (singular value decomposition). We'll look at those & preproc steps affects if time allows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e41cde9",
   "metadata": {},
   "source": [
    "## Embedding Model Time\n",
    "\n",
    "Here we are treating each doc (since they are very short) as a single sentence, just for simplicity. If time allows, we'd check this in the data exploration step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "39e3180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df['lemmatized'].apply(lambda x: x.split())\n",
    "w2vModel = Word2Vec( df['tokenized'], vector_size=100, window=5, min_count=3, workers=10)\n",
    "w2vSvmModel = svm.SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "332a6bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(w2vModel.wv.index_to_key)\n",
    "\n",
    "#Turns out with stop word removal some items have 0 words so deleted that out of preprocessing.\n",
    "#Also turns out some items only have 2 word phrases, so deleted that out of preprocessing.\n",
    "badsDf = df[df['tokenized'].apply(lambda x: len(x) < 1)] \n",
    "badsDf.head()\n",
    "\n",
    "def calc_vec(tokensLst):\n",
    "    vecs = [w2vModel.wv[word] for word in tokensLst if word in vocab]\n",
    "    if len(vecs) == 0:\n",
    "        vecs = [np.zeros(100), np.zeros(100)]\n",
    "    return sum(vecs) / max([len(tokensLst), 1])\n",
    "\n",
    "df['w2vVectors'] = df['tokenized'].apply(lambda x: calc_vec(x))\n",
    "X = list( df['w2vVectors'] )\n",
    "y = list( df['label'] )\n",
    "\n",
    "w2vSvmModel.fit( X, y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9e77b05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03447150574248681"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['tokenized'] = test_df['lemmatized'].apply(lambda x: x.split())\n",
    "test_df['w2vVectors'] = test_df['tokenized'].apply(lambda x: calc_vec(x))\n",
    "testX = list( test_df['w2vVectors'] )\n",
    "testy = list( test_df['label'] )\n",
    "testPredy = w2vSvmModel.predict( testX )\n",
    "w2vSvmModel.score( testX, testy )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1085ad86",
   "metadata": {},
   "source": [
    "Interestingly, much worse performance than the non-embedding models. I wonder why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf58ea4",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "\n",
    "- Examine using PCA & SVD w. BOW-esk models\n",
    "- Try FastText\n",
    "- Figure out how many reviews are a single sentence\n",
    "- Compare results of stemming instead of lemmatization\n",
    "- Use RMSE in addition to R^2 \n",
    "- Use confusion Matrices\n",
    "- Look at distro of lengths of reviews\n",
    "- Include pretrained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4422d72f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
